Contextualisation
Ce document présente un exemple de mise en forme pour le projet Word du cours d’Administration. Pour plus d’information sur le travail demandé, se référer à l’énoncé du projet.
Le contenu de ce document est une adaptation libre de divers extraits trouvés sur Internet (principalement cet article disponible sur Wikipedia).
Définition
Le terme intelligence artificielle, créé par John McCarthy, est souvent abrégé par le sigle IA (ou AI en anglais, pour Artificial Intelligence). Il est défini par l’un de ses créateurs, comme : 
« ... la construction de programmes informatiques qui s’adonnent à des tâches qui sont, pour l’instant, accomplies de façon plus satisfaisante par des êtres humains, car elles demandent des processus mentaux de haut niveau tels que : l’apprentissage perceptuel, l’organisation de la mémoire et le raisonnement critique. »
Marvin Lee Minsky
On y trouve donc le côté artificiel atteint par l'usage des ordinateurs ou de processus électroniques élaborés et le côté intelligence associé à son but d'imiter le comportement. Cette imitation peut se faire dans le raisonnement, par exemple dans les jeux ou la pratique de mathématiques, dans la compréhension des langues naturelles, dans la perception : visuelle (interprétation des images et des scènes), auditive (compréhension du langage parlé) ou par d'autres capteurs, dans la commande d'un robot dans un milieu inconnu ou hostile.
Même si elles respectent globalement la définition de Minsky, il existe un certain nombre de définitions différentes de l'IA qui varient sur deux points fondamentaux :
Les définitions qui lient la définition de l'IA à un aspect humain de l'intelligence, et celles qui la lient à un modèle idéal d'intelligence, non forcément humaine, nommée rationalité.
Les définitions qui insistent sur le fait que l'IA a pour but d'avoir toutes les apparences de l'intelligence (humaine ou rationnelle), et celles qui insistent sur le fait que le fonctionnement interne du système d'IA doit ressembler également à celui de l'être humain ou être rationnel.
Histoire
L'origine de l'intelligence artificielle se trouve probablement dans l'article d'Alan Turing Computing Machinery and Intelligence (1950), où Turing explore le problème et propose une expérience maintenant connue sous le nom de test de Turing dans une tentative de définition d'un standard permettant de qualifier une machine de consciente. Il développe ces idées dans plusieurs forums et conférences dès 1951.
On considère que l'intelligence artificielle, en tant que domaine de recherche, a été créée à la conférence qui s'est tenue sur le campus de Dartmouth College pendant l'été 1956 à laquelle assistaient ceux qui vont marquer la discipline. Ensuite l'intelligence artificielle se développe surtout aux États-Unis à l'université Stanford sous l'impulsion de John McCarthy, au MIT sous celle de Marvin Minsky, à l'université Carnegie-Mellon sous celle de Allen Newell et Herbert Simon et à l'Université d'Édimbourg sous celle de Donald Michie.
 
Alan Turing (1912-1954) est considéré comme le fondateur de la science informatique.
Deux types d’intelligence artificielle
Intelligence artificielle forte
Le concept d’intelligence artificielle forte fait référence à une machine capable non seulement de produire un comportement intelligent, mais d’éprouver une impression d'une réelle conscience de soi, de vrais sentiments (quoi qu’on puisse mettre derrière ces mots), et une compréhension de ses propres raisonnements.
L’intelligence artificielle forte a servi de moteur à la discipline, mais a également suscité de nombreux débats. En se fondant sur le constat que la conscience a un support biologique et donc matériel, les scientifiques ne voient généralement pas d’obstacle de principe à créer un jour une intelligence consciente sur un support matériel autre que biologique. Selon les tenants de l'IA forte, si à l'heure actuelle il n'y a pas d'ordinateurs ou de robots aussi intelligents que l'être humain, ce n'est pas un problème d'outil, mais de conception. Il n'y aurait aucune limite fonctionnelle (un ordinateur est une machine de Turing universelle avec pour seules limites les limites de la calculabilité), il n'y aurait que des limites liées à l'aptitude humaine à concevoir les logiciels appropriés (programme, base de données...). Elle permet notamment de modéliser des idées abstraites.
Intelligence artificielle faible
La notion d’intelligence artificielle faible constitue une approche pragmatique d’ingénieur : chercher à construire des systèmes de plus en plus autonomes (pour réduire le coût de leur supervision), des algorithmes capables de résoudre des problèmes d’une certaine classe, etc. Mais, cette fois, la machine simule l'intelligence, elle semble agir comme si elle était intelligente. 
On en voit des exemples concrets avec les programmes conversationnels qui tentent de passer le test de Turing, comme ELIZA. Ces logiciels parviennent à imiter de façon grossière le comportement d'humains face à d'autres humains lors d'un dialogue.
Un exemple
Exemple d’un programme d’intelligence artificielle (pseudocode basé sur le langage C).
solution AI(problem_to_solve)
{
	if (knowledge >= problem_to_solve) {
		return solve(problem_to_solve);
	} else {
		knowledge += gatherKnowledge(problem_to_solve);
		return AI(problem_to_solve);
	}
}
Courants de pensée
La cybernétique naissante des années 1940 revendiquait très clairement son caractère pluridisciplinaire et se nourrissait des contributions les plus diverses : neurophysiologie, psychologie, logique, sciences sociales… Et c’est tout naturellement qu’elle envisagea deux approches des systèmes, deux approches reprises par les sciences cognitives et de ce fait l’intelligence artificielle : une approche par la décomposition (du haut vers le bas) et une approche contraire par construction progressive du bas vers le haut.
Ces deux approches se révèlent plutôt complémentaires que contradictoires : on est à l'aise pour décomposer rapidement ce que l'on connaît bien, et une approche pragmatique à partir des seuls éléments que l'on connaît afin de se familiariser avec les concepts émergents est plus utile pour les domaines inconnus. Elles sont respectivement à la base des hypothèses de travail que constituent le cognitivisme et le connexionnisme, qui tentent aujourd'hui d'opérer progressivement leur fusion.
Certains courants de pensée sur l'intelligence artificielle adoptent pour la commodité du lecteur la taxinomie suivante :
Systèmes symboliques
Connexionnisme
Calcul évolutif (algorithmes génétiques, par exemple)
Alife (vie artificielle) et complexité
Agents et robotique
Trois concepts reviennent de façon récurrente dans la plupart des travaux :
la redondance (le système est peu sensible à des pannes ponctuelles)
la réentrance (les composants s'informent en permanence entre eux)
la sélection (au fil du temps, les comportements efficaces sont dégagés et renforcés)
Utilisation
Domaines d’application
L'intelligence artificielle a été et est utilisée (ou intervient) dans une variété de domaines tels que :
bancaire, avec des systèmes experts d'évaluation de risque lié à l'octroi d'un crédit;
militaire, avec les systèmes autonomes tels que les drones, les systèmes de commandement et l'aide à la décision;
les jeux, avec les personnages artificiels évoluant dans les différents scénarios de jeu;
la médecine, avec les systèmes experts d'aide au diagnostic;
la logistique, au travers d'approches heuristiques de type résolution de problème de satisfaction de contraintes;
l’ingénierie et la recherche scientifique, pour la recherche de solutions pertinentes à des problèmes complexes et aussi variés que le calcul de trajectoire ou l’optimisation d’un système d’inspection autonome.
 
Roadrunner, fabriqué par la compagnie IBM,  est le premier superordinateur à avoir officiellement dépassé la puissance de 1 pétaflops, 2008
Jeux vidéo
L'intelligence artificielle a par exemple été utilisée depuis longtemps dans la conception de joueurs artificiels pour le jeu d'échecs. Toutefois, c'est dans les jeux vidéo que l'intelligence artificielle s'est le plus popularisée, et c'est aussi un des domaines où elle se développe rapidement. Celle-ci bénéficie en effet des progrès de l'informatique, avec par exemple les cartes graphiques dédiées qui déchargent le processeur principal des tâches graphiques. Le processeur principal peut désormais être utilisé pour développer des systèmes d’IA plus perfectionnés. Par exemple, l'intelligence artificielle peut être utilisée pour piloter des bots (c'est-à-dire les personnages artificiels) évoluant dans les MMOGs ou les mondes virtuels, mais on peut aussi citer son utilisation dans des jeux de simulation, ou pour animer des personnages artificiels.
Dans le domaine du jeu vidéo, l’IA caractérise toute prise de décision d’un personnage (ou d’un groupe) géré par le jeu, et contraint par l’intérêt ludique : une meilleure IA ne donne pas forcément un jeu plus jouable, l’objectif est de donner l’illusion d’un comportement intelligent. L'éventail des sujets (recherche de chemin, animation procédurale, planifications stratégiques…) est réalisable par différentes techniques classiques issues de deux paradigmes distincts : IA symbolique (automates, script, systèmes multiagents…), et IA située (réseau de neurones, algorithmes évolutionnistes…) ; où l’une est fortement dépendante de l’expertise humaine, et l’autre de l’expérience en situation. La première approche est globalement préférée, car mieux contrôlée; la deuxième est préférée pour certains comportements (déplacement d’une formation, désirs, satisfactions). Elles partagent toutes les mêmes contraintes de ressources restreintes, que ce soit en mémoire, en temps de développement, ou en temps de calcul, même si globalement ces ressources augmentent progressivement.
Jusqu'à la fin des années 1990, l’IA dans les jeux vidéo (plus particulièrement dans les jeux en temps réel) a été délaissée par rapport au rendu visuel et sonore. L’évolution vers des univers toujours plus réalistes, leur peuplement par des personnages aux comportements crédibles devient une problématique importante. Pour éviter ce contraste, et coupler dans le même temps au délestage d’une grosse partie de l’aspect graphique des processeurs vers les cartes graphiques, on constate à cette période une augmentation des ressources investies dans l’IA (temps de développement, ressource processeur). Certains jeux sont précurseurs, car l’IA y constitue l’élément central ludique. Partant d’une approche à base de règles rigides, les jeux utilisent alors des IA plus flexibles, diversifiant les techniques mises en œuvre. Aujourd'hui la plupart des jeux vidéo utilisent des solutions ad hoc, il existe néanmoins des solutions middleware et également des solutions matérielles toutefois très minoritaires.
Avec les jeux en réseau, le besoin d’IA a tout d’abord été négligé, mais, particulièrement avec l’apparition des jeux massivement multijoueurs, et la présence d’un nombre très important de joueurs humains se confrontant à des personnages non joueur, ces derniers ont un besoin très important de pouvoir s'adapter à des situations qui ne peuvent être prévues. Actuellement ces types de jeux intéressent particulièrement des chercheurs en IA, y trouvant un environnement adéquat pour y éprouver différentes architectures adaptatives.
Questions soulevées
Essor
L’intelligence artificielle a connu un essor important de 1960 à 1970, mais à la suite de résultats décevants par rapport aux capitaux investis dans le domaine, son succès s’estompa dès le milieu des années 1980. Par ailleurs, un certain nombre de questions se posent telles que la possibilité un jour pour les robots d'accéder à la conscience, ou d'éprouver des émotions.
 
ASIMO (Advanced Step in Innovative Mobility) est un robot humanoïde servant à la recherche développé par l’entreprise japonaise Honda. D’après ses créateurs, ASIMO aurait, dès sa 3e version, un niveau d’intelligence comparable à un enfant de trois ans et une habileté physique d’un enfant de 10 ans. Aujourd’hui, à la 5e version, il peut courir à 9 km/h, éviter les obstacles (statiques et dynamiques), s’adapter à son environnement et effectuer du travail collaboratif avec plusieurs robots. Les ingénieurs de Honda ont même développé un appareil permettant de le contrôler par la pensée humaine.
D’après certains auteurs, les perspectives de l’intelligence artificielle pourraient avoir des inconvénients, si par exemple les machines devenaient plus intelligentes que les humains, et finissaient par les dominer, voire (pour les plus pessimistes, les exterminer), de la même façon que nous cherchons à exterminer certaines séquences d’ARN (les virus) alors que nous sommes construits à partir d'ADN, un proche dérivé de l'ARN. On reconnaît le thème du film Terminator, mais des directeurs de société techniquement très compétents, comme Bill Joy de la société Sun, affirment considérer le risque comme réel à long terme. Toutes ces possibilités futures ont fait l’objet de quantités de romans de science-fiction, tels ceux d’Isaac Asimov ou William Gibson en passant par Arthur C. Clarke.
Espoirs et méfiances
Une description spectaculaire d’un possible avenir de l’intelligence artificielle a été faite un professeur très connu dans le domaine :
« Supposons qu’existe une machine surpassant en intelligence tout ce dont est capable un homme, aussi brillant soit-il. La conception de telles machines faisant partie des activités intellectuelles, cette machine pourrait à son tour créer des machines meilleures qu’elle-même ; cela aurait sans nul doute pour effet une réaction en chaîne de développement de l’intelligence, pendant que l’intelligence humaine resterait presque sur place. Il en résulte que la machine ultra intelligente sera la dernière invention que l’homme aura besoin de faire, à condition que ladite machine soit assez docile pour constamment lui obéir. »
I. J. Good
La situation en question, correspondant à un changement qualitatif du principe même de progrès, a été nommée La Singularité. Ce concept est central pour de nombreux transhumanistes, qui s'interrogent très sérieusement sur les dangers et les espoirs liés à un tel scénario, certains allant jusqu'à envisager l'émergence d'un dieu numérique appelé à prendre le contrôle du destin de l'humanité, ou à fusionner avec elle.
Good estimait à un peu plus d'une chance sur deux la mise au point d'une telle machine avant la fin du XXe siècle. La prédiction, en 2012, ne s’est toujours pas réalisée, mais avait imprégné le public à l'époque : le cours de l’action d'IBM quadrupla (bien que les dividendes trimestriels versés restèrent à peu de chose près les mêmes) dans les mois qui suivirent la victoire de Deep Blue sur Garry Kasparov. Une large partie du grand public était en effet persuadée qu’IBM venait de mettre au point le vecteur d’une telle explosion de l’intelligence et que cette compagnie en tirerait profit. L’espoir fut déçu : une fois sa victoire acquise, Deep Blue, simple calculateur évaluant 200 millions de positions à la seconde, sans conscience du jeu lui-même, fut reconverti en machine classique utilisée pour l'exploration de données. Nous sommes probablement encore très loin d’une machine possédant ce que nous nommons de l'intelligence générale, et tout autant d’une machine possédant la base de connaissances de n’importe quel chercheur, si humble soit-il.

